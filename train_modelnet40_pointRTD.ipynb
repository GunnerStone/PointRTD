{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Import Libraries ---\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pointRTD import PointRTDModel  # Import the PointRTD model\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from plyfile import PlyData\n",
    "import trimesh  # For loading .off files as point clouds\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm  # tqdm for Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Define Hyperparameters ---\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 300\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.05\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 40\n",
    "CORRUPTION_RATIO = 0.6\n",
    "CHECKPOINT_DIR = f\"./checkpoints_modelnet40/PointRTD/CR_{CORRUPTION_RATIO}\"\n",
    "PRETRAINED_CHECKPOINT = f\"./checkpoints_pointrtd/pointrtd_epoch_62_CR_{CORRUPTION_RATIO}.pth\"\n",
    "LOG_DIR = \"./tensorboard_logs_modelnet10\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "print(\"Device: \", DEVICE)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNet40Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', random_split=False, num_points=1024, seed=42, augment=False):\n",
    "        \"\"\"\n",
    "        ModelNet40 dataset class.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing ModelNet40.\n",
    "            split (str): 'train' or 'test'.\n",
    "            random_split (bool): If True, use random data split instead of the preset split.\n",
    "            num_points (int): Number of points to sample from each point cloud.\n",
    "            seed (int): Random seed for reproducibility.\n",
    "            augment (bool): Apply data augmentation if True.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.num_points = num_points\n",
    "        self.augment = augment\n",
    "        self.data = []\n",
    "        \n",
    "        # Set up random split if specified\n",
    "        if random_split:\n",
    "            self.random_seed_split(seed)\n",
    "        else:\n",
    "            self.preset_split()\n",
    "    \n",
    "    def preset_split(self):\n",
    "        \"\"\"Use preset train/test split from ModelNet40.\"\"\"\n",
    "        classes = sorted(os.listdir(self.root_dir))\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        for cls_name in classes:\n",
    "            class_path = os.path.join(self.root_dir, cls_name, self.split)\n",
    "            for file_name in os.listdir(class_path):\n",
    "                if file_name.endswith('.off'):\n",
    "                    self.data.append((os.path.join(class_path, file_name), self.class_to_idx[cls_name]))\n",
    "    \n",
    "    def random_seed_split(self, seed):\n",
    "        \"\"\"Create a random split by shuffling files.\"\"\"\n",
    "        random.seed(seed)\n",
    "        classes = sorted(os.listdir(self.root_dir))\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        for cls_name in classes:\n",
    "            class_files = os.listdir(os.path.join(self.root_dir, cls_name))\n",
    "            random.shuffle(class_files)\n",
    "            split_idx = int(len(class_files) * 0.8)\n",
    "            if self.split == 'train':\n",
    "                self.data.extend([(os.path.join(self.root_dir, cls_name, f), self.class_to_idx[cls_name]) for f in class_files[:split_idx]])\n",
    "            else:\n",
    "                self.data.extend([(os.path.join(self.root_dir, cls_name, f), self.class_to_idx[cls_name]) for f in class_files[split_idx:]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, class_idx = self.data[idx]\n",
    "        mesh = trimesh.load(file_path)\n",
    "        points = mesh.sample(self.num_points)\n",
    "        points = np.array(points, dtype=np.float32)\n",
    "        \n",
    "        # Apply augmentation if specified\n",
    "        if self.augment and self.split == 'train':\n",
    "            points = self.apply_augmentations(points)\n",
    "        \n",
    "        return torch.tensor(points, dtype=torch.float32), class_idx\n",
    "\n",
    "    def apply_augmentations(self, points):\n",
    "        \"\"\"Random scaling and translation.\"\"\"\n",
    "        scale = np.random.uniform(0.8, 1.2)\n",
    "        points *= scale\n",
    "        points += np.random.uniform(-0.1, 0.1, size=(1, 3))\n",
    "        return points\n",
    "    \n",
    "# --- 4. Initialize Dataloaders ---\n",
    "root_dir = './ModelNet40'\n",
    "train_dataset = ModelNet40Dataset(root_dir, split='train', random_split=False, augment=True)\n",
    "test_dataset = ModelNet40Dataset(root_dir, split='test', random_split=False, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./checkpoints_pointrtd/pointrtd_epoch_62_CR_0.6.pth\n",
      "Checkpoint loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Load Pretrained Model and Set Up Classifier ---\n",
    "# Load pretrained PointMAE model\n",
    "token_dim = 256\n",
    "hidden_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "num_patches = 64\n",
    "num_pts_per_patch = 32\n",
    "num_channels = 3\n",
    "corruption_ratio = CORRUPTION_RATIO\n",
    "noise_scale = 1\n",
    "\n",
    "# Initialize PointRTD Model\n",
    "pointrtd_model = PointRTDModel(\n",
    "    input_dim=num_channels,\n",
    "    token_dim=token_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    corruption_ratio=corruption_ratio,\n",
    "    noise_scale=noise_scale,\n",
    "    num_patches=num_patches,\n",
    "    num_pts_per_patch=num_pts_per_patch,\n",
    "    finetune=True, # Disable masking \n",
    ").to(DEVICE)\n",
    "\n",
    "if os.path.isfile(PRETRAINED_CHECKPOINT):\n",
    "    print(f\"Loading checkpoint from {PRETRAINED_CHECKPOINT}\")\n",
    "    state_dict = torch.load(PRETRAINED_CHECKPOINT, map_location=DEVICE, weights_only=True)['model_state_dict']\n",
    "    pointrtd_model.load_state_dict(state_dict)\n",
    "    print(\"Checkpoint loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at {PRETRAINED_CHECKPOINT}\")\n",
    "\n",
    "encoder = pointrtd_model.encoder\n",
    "\n",
    "class EncoderWithClassifier(nn.Module):\n",
    "    def __init__(self, encoder, token_dim=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(token_dim * 3, num_classes, bias=False)  # token_dim * 3 due to concatenation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode the input to get tokens\n",
    "        encoded_tokens, *_ = self.encoder(x)  # Shape: (B, num_patches, token_dim)\n",
    "        \n",
    "        # Extract CLS token (first token)\n",
    "        cls_token = encoded_tokens[:, 0, :]  # Shape: (B, token_dim)\n",
    "\n",
    "        # Mean pooling across all tokens\n",
    "        mean_pooled = encoded_tokens.mean(dim=1)  # Shape: (B, token_dim)\n",
    "\n",
    "        # Max pooling across all tokens\n",
    "        max_pooled, _ = encoded_tokens.max(dim=1)  # Shape: (B, token_dim)\n",
    "\n",
    "        # Concatenate CLS token, mean-pooled, and max-pooled features\n",
    "        combined_features = torch.cat([cls_token, mean_pooled, max_pooled], dim=-1)  # Shape: (B, token_dim * 3)\n",
    "\n",
    "        # Pass through the classifier head\n",
    "        logits = self.classifier(combined_features)  # Shape: (B, num_classes)\n",
    "        return logits\n",
    "\n",
    "classification_model = EncoderWithClassifier(encoder, token_dim=256, num_classes=NUM_CLASSES).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts: [626, 106, 515, 173, 572, 335, 64, 197, 889, 167, 79, 138, 200, 109, 200, 149, 171, 155, 145, 124, 149, 284, 465, 200, 88, 231, 240, 104, 115, 128, 680, 124, 90, 392, 163, 344, 267, 475, 87, 103]\n",
      "Class Weights: [15.723642172523961, 92.85849056603773, 19.1126213592233, 56.895953757225435, 17.208041958041957, 29.382089552238806, 153.796875, 49.964467005076145, 11.071991001124859, 58.94011976047904, 124.59493670886076, 71.32608695652173, 49.215, 90.30275229357798, 49.215, 66.06040268456375, 57.56140350877193, 63.50322580645161, 67.88275862068966, 79.37903225806451, 66.06040268456375, 34.65845070422535, 21.16774193548387, 49.215, 111.85227272727273, 42.61038961038961, 41.0125, 94.64423076923077, 85.59130434782608, 76.8984375, 14.475, 79.37903225806451, 109.36666666666666, 25.10969387755102, 60.38650306748466, 28.613372093023255, 36.86516853932584, 20.722105263157896, 113.13793103448276, 95.5631067961165]\n"
     ]
    }
   ],
   "source": [
    "# --- Calculate class distribution for cross entropy class weights\n",
    "# --- MAY TAKE A WHILE\n",
    "# from collections import Counter\n",
    "# import torch\n",
    "\n",
    "# # Step 1: Initialize a counter\n",
    "# class_counts = Counter()\n",
    "\n",
    "# # Step 2: Count the occurrences of each class in the training dataset\n",
    "# for _, label in train_dataset:\n",
    "#     class_counts[label] += 1\n",
    "\n",
    "# # Step 3: Convert counts to a list\n",
    "# num_classes = len(class_counts)\n",
    "# class_count_list = [class_counts[i] for i in range(num_classes)]\n",
    "class_count_list = [626, 106, 515, 173, 572, 335, 64, 197, 889, 167, 79, 138, 200, 109, 200, 149, 171, 155, 145, 124, 149, 284, 465, 200, 88, 231, 240, 104, 115, 128, 680, 124, 90, 392, 163, 344, 267, 475, 87, 103] # Precomputed\n",
    "\n",
    "# Step 4: Compute class weights (optional, for use in CrossEntropyLoss)\n",
    "total_samples = sum(class_count_list)\n",
    "class_weights = [total_samples / count for count in class_count_list]\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "print(\"Class Counts:\", class_count_list)\n",
    "print(\"Class Weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Define Optimizer and Scheduler ---\n",
    "optimizer = optim.AdamW(classification_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Define Training Loop with tqdm ---\n",
    "def train_one_epoch(model, loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(loader), total=len(loader), desc=f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "    \n",
    "    # Initialize CrossEntropy loss\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    for batch_idx, (points, labels) in progress_bar:\n",
    "        points, labels = points.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(points)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update tqdm progress bar\n",
    "        progress_bar.set_postfix(Batch=f\"{batch_idx+1}/{len(loader)}\", Loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, loader, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Initialize CrossEntropy loss\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(loader), total=len(loader), desc=f\"Validation Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "        for batch_idx, (points, labels) in progress_bar:\n",
    "            points, labels = points.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(points)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Update tqdm progress bar\n",
    "            progress_bar.set_postfix(Batch=f\"{batch_idx+1}/{len(loader)}\", Loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b968f29ac1c4560bcf4f7a6c360aef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [1/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/cc-filer/home/gunnerstone/anaconda3/envs/pytorch3d/lib/python3.10/site-packages/trimesh/grouping.py:99: RuntimeWarning: invalid value encountered in cast\n",
      "  stacked = np.column_stack(stacked).round().astype(np.int64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d423b1dcb3f34edc94d357ee43fd63dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [1/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] - Train Loss: 3.7254, Train Acc: 0.0617, Val Loss: 3.8695, Val Acc: 0.0543\n",
      "New best model saved with Val Loss: 3.8695 at epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d83fe556c894f86b01693c6ba1f87e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [2/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c32766a823b4531bfb6637afe25c007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [2/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300] - Train Loss: 3.3452, Train Acc: 0.1390, Val Loss: 3.4053, Val Acc: 0.1090\n",
      "New best model saved with Val Loss: 3.4053 at epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1941269aae6440ec890b0be1cd8a4c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [3/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2e23fa0f314659bdfcda1509139c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [3/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300] - Train Loss: 2.9239, Train Acc: 0.2326, Val Loss: 3.1765, Val Acc: 0.1718\n",
      "New best model saved with Val Loss: 3.1765 at epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e7450a88744f86920a5a4b96980024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [4/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2f4180f2ac4b2cbd4a36c25d03cb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [4/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300] - Train Loss: 2.5840, Train Acc: 0.3116, Val Loss: 2.7125, Val Acc: 0.3035\n",
      "New best model saved with Val Loss: 2.7125 at epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c7fbf871f04740907b5e385601d881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [5/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d06630c3814ee5b6cf599b02cf16b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [5/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300] - Train Loss: 2.3352, Train Acc: 0.3751, Val Loss: 2.2632, Val Acc: 0.3975\n",
      "New best model saved with Val Loss: 2.2632 at epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6a4abffff84e3797457e7e504c716e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [6/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410dff11953c42cba41a160b2c0a357a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [6/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300] - Train Loss: 2.1153, Train Acc: 0.4357, Val Loss: 2.4116, Val Acc: 0.3578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df929da3a4c54340bae90a584a32385e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [7/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be58edb3291435f9f27d6e7b8a7efb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [7/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300] - Train Loss: 2.0530, Train Acc: 0.4630, Val Loss: 2.3565, Val Acc: 0.4007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c6b1fce6204873831d5bcc8227c20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [8/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe93ec11d4c4d97b2c9ad06ae4820c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [8/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300] - Train Loss: 1.8834, Train Acc: 0.5046, Val Loss: 1.8305, Val Acc: 0.4765\n",
      "New best model saved with Val Loss: 1.8305 at epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99349928b548469f95ec017e13f5108f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [9/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7db31129e6e4da6a819bc7924212157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [9/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300] - Train Loss: 1.6472, Train Acc: 0.5628, Val Loss: 1.7747, Val Acc: 0.5142\n",
      "New best model saved with Val Loss: 1.7747 at epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7814522f871f4b578a5218d92fc96353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [10/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb6ee7b6be44d8e8f87a7074d8dc5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [10/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300] - Train Loss: 1.5762, Train Acc: 0.5873, Val Loss: 1.7935, Val Acc: 0.5008\n",
      "Checkpoint saved at ./checkpoints_modelnet40/PointRTD/CR_0.6/classification_epoch_10.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b9619d38a046b69e450dc4360a2f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [11/300]:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe228419ea94d19bd80d8d76f6df078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch [11/300]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a variable to keep track of the lowest validation loss\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1  # To track the epoch of the best model\n",
    "\n",
    "# --- 8. Training and Validation Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(classification_model, train_loader, optimizer, epoch)\n",
    "    val_loss, val_acc = validate_one_epoch(classification_model, test_loader, epoch)\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar(\"Train/Loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Train/Accuracy\", train_acc, epoch)\n",
    "    writer.add_scalar(\"Validation/Loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Validation/Accuracy\", val_acc, epoch)\n",
    "    writer.add_scalar(\"Learning_Rate\", scheduler.get_last_lr()[0], epoch)\n",
    "\n",
    "    # Print epoch stats\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"classification_epoch_{epoch+1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': classification_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch + 1  # Update to the current epoch (1-based index)\n",
    "        best_checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"best_model_epoch_{best_epoch}.pth\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': classification_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()\n",
    "        }, best_checkpoint_path)\n",
    "\n",
    "        print(f\"New best model saved with Val Loss: {val_loss:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# --- Load Classification Model Checkpoint ---\n",
    "def load_classification_checkpoint(model, checkpoint_path):\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Checkpoint loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "\n",
    "\n",
    "# --- Evaluate Model on Test Set with Voting ---\n",
    "def evaluate_model_with_voting(model, loader, num_votes=10, class_names=None):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for points, labels in tqdm(loader, desc=\"Evaluating Test Set with Voting\"):\n",
    "            points, labels = points.to(DEVICE), labels.to(DEVICE)\n",
    "            batch_size = points.size(0)\n",
    "            vote_predictions = []\n",
    "\n",
    "            for v in range(num_votes):\n",
    "                # Apply random augmentations to the points\n",
    "                augmented_points = apply_test_augmentations(points.clone())\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(augmented_points)\n",
    "                _, predicted = outputs.max(1)\n",
    "                vote_predictions.append(predicted.cpu().numpy())\n",
    "\n",
    "            # Majority voting\n",
    "            vote_predictions = np.array(vote_predictions)  # Shape: (num_votes, batch_size)\n",
    "            final_predictions = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                votes = vote_predictions[:, i]\n",
    "                most_common = Counter(votes).most_common(1)[0][0]\n",
    "                final_predictions.append(most_common)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(final_predictions)\n",
    "\n",
    "    # Calculate accuracy and F1 score\n",
    "    test_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    test_f1_score = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    # Calculate and optionally display confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    if class_names is not None:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "\n",
    "    return test_accuracy, test_f1_score, conf_matrix\n",
    "\n",
    "def apply_test_augmentations(points):\n",
    "    \"\"\"Apply random rotations and jittering for test-time augmentation.\"\"\"\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Evaluation Loop ---\n",
    "checkpoint_epochs = [10, 50, 100, 150, 200, 250, 300]\n",
    "checkpoint_epochs.reverse()\n",
    "results = []\n",
    "\n",
    "class_names = [\n",
    "    'airplane', 'bathtub', 'bed', 'bench', 'bookshelf', 'bottle', 'bowl', \n",
    "    'car', 'chair', 'cone', 'cup', 'curtain', 'desk', 'door', 'dresser', \n",
    "    'flower_pot', 'glass_box', 'guitar', 'keyboard', 'lamp', 'laptop', \n",
    "    'mantel', 'monitor', 'night_stand', 'person', 'piano', 'plant', \n",
    "    'radio', 'range_hood', 'sink', 'sofa', 'stairs', 'stool', 'table', \n",
    "    'tent', 'toilet', 'tv_stand', 'vase', 'wardrobe', 'xbox'\n",
    "]\n",
    "\n",
    "for epoch in checkpoint_epochs:\n",
    "    checkpoint_path = f\"./checkpoints_modelnet10/PointRTD/CR_{CORRUPTION_RATIO}/classification_epoch_{epoch}.pth\"\n",
    "    # checkpoint_path = f\"./checkpoints_modelnet10/PointRTD/CR_{CORRUPTION_RATIO}/best_model_epoch_106.pth\"\n",
    "    print(f\"\\nEvaluating Model at Epoch {epoch}...\")\n",
    "    \n",
    "    try:\n",
    "        load_classification_checkpoint(classification_model, checkpoint_path)\n",
    "        test_accuracy_voting, test_f1_score_voting, conf_matrix = evaluate_model_with_voting(\n",
    "            classification_model, test_loader, num_votes=10, class_names=class_names\n",
    "        )\n",
    "        print(f\"Accuracy: {test_accuracy_voting}; F1 Score: {test_f1_score_voting};\")\n",
    "        results.append((epoch, test_accuracy_voting, test_f1_score_voting))\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"Epoch\\tAccuracy\\tF1 Score\")\n",
    "for epoch, acc, f1 in results:\n",
    "    print(f\"{epoch}\\t{acc:.4f}\\t\\t{f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
